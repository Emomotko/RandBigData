---
title: "Praca domowa 02"
author: "Katarzyna Fąk"
date: "18.03.2015"
output: html_document
---

Wymagane biblioteki:
```{r,results='hide'}
library(twitteR)
library(streamR)
library(ROAuth)
library(stringi)
library(wordcloud)
```

Twitty pobierałam w dwóch turach (czy istnieje sposób nasłuchiwania twitów bez blokowania pracy RStudio?).

```{r,cache=TRUE}
# requestURL <- "https://api.twitter.com/oauth/request_token"
# accessURL <- "https://api.twitter.com/oauth/access_token"
# authURL <- "https://api.twitter.com/oauth/authorize"
# consumerKey    <- "5gUNkfuwFsbRpZ8DWc2sm1er5"
# consumerSecret <- "At4GorGYyKJA9ziU9ERjs0viWFLCNU7OssaE2Sd3MeD6Gsqunh"
# access_token   <- "2236709701-TjdruvDxyQjrVAc2mSdALKPxBouMGGYgbqexkC3"
# access_secret  <- "fNE9QVe7EqdoyccXNob7oan0bfo4YB5HyHltTzvuIrppv"
# 
# # proces autoryzacji
# paczka <- OAuthFactory$new(consumerKey = consumerKey, consumerSecret = consumerSecret, 
#                            oauthKey = access_token, oauthSecret = access_secret,
#                            requestURL = requestURL, accessURL = accessURL, authURL = authURL)
# 
# 
# paczka$handshake(cainfo = system.file("CurlSSL", "cacert.pem", package = "RCurl"))
# 
# data <- strftime(Sys.time(),"%Y-%m-%d@%H:%M")
# filterStream( file = file.path(paste0(data,".json")),
#               timeout = 100*60,
#               #language = "PL",
#               locations = c(14.07,49,24,54.5),
#               oauth=paczka) # tu ustawić polske!!

parsedTwees <- parseTweets("2015-03-18@17-33.json", simplify = FALSE, verbose = TRUE)
parsedTwees2 <- parseTweets("2015-03-18@19-55.json", simplify = FALSE, verbose = TRUE)
```

Interesują nas twitty w języku polskim:

```{r, cache=TRUE}
twitty <- parsedTwees[parsedTwees$country_code=="PL",]$text
twitty <- unique(c(twitty,parsedTwees2[parsedTwees2$country_code=="PL",]$text))
```

Czyścimy twitty ze znaków specjalnych: @, #, RT oraz nazw użytkowników.

```{r, cache=TRUE}
      data <- stri_replace_all_regex(twitty, "(RT|via)((?:\\b\\W*@\\w+)+)","") # usuwanie RT z poczatkow
      data <- stri_replace_all_regex(data, "@\\w+","")                         # usuwanie nazw @...
      data <- stri_replace_all_regex(data, "http(s)*://[^ ]+","")              # usuwanie linkow
      data <- stri_replace_all_regex(data, "#[^ ]+","")                        # usuwanie nazw #...
      data <- stri_trans_tolower(data)
      data <- unlist(stri_extract_all_words(data))
```

Usuwamy słowa bez znaczenia: zaimki, spójniki, itd. Dane ze strony [wikipedia.org](http://pl.wikipedia.org/wiki/Wikipedia:Stopwords) uzupełniłam kilkoma własnymi spostrzeżeniami.

```{r, cache=TRUE,echo=FALSE}
stopwords <- "a, aby, ach, acz, aczkolwiek, aj, albo, ale, ależ, ani, aż, bardziej, bardzo, bo, bowiem, by, byli, bynajmniej, być, był, była, było, były, będzie, będą, cali, cała, cały, ci, cię, ciebie, co, cokolwiek, coś, czasami, czasem, czemu, czy, czyli, daleko, dla, dlaczego, dlatego, do, dobrze, dokąd, dość, dużo, dwa, dwaj, dwie, dwoje, dziś, dzisiaj, gdy, gdyby, gdyż, gdzie, gdziekolwiek, gdzieś, i, ich, ile, im, inna, inne, inny, innych, iż, ja, ją, jak, jakaś, jakby, jaki, jakichś, jakie, jakiś, jakiż, jakkolwiek, jako, jakoś, je, jeden, jedna, jedno, jednak, jednakże, jego, jej, jemu, jest, jestem, jeszcze, jeśli, jeżeli, już, ją, każdy, kiedy, kilka, kimś, kto, ktokolwiek, ktoś, która, które, którego, której, który, których, którym, którzy, ku, lat, lecz, lub, ma, mają, mało, mam, mi, mimo, między, mną, mnie, mogą, moi, moim, moja, moje, może, możliwe, można, mój, mu, musi, my, na, nad, nam, nami, nas, nasi, nasz, nasza, nasze, naszego, naszych, natomiast, natychmiast, nawet, nią, nic, nich, nie, niech, niego, niej, niemu, nigdy, nim, nimi, niż, no, o, obok, od, około, on, ona, one, oni, ono, oraz, oto, owszem, pan, pana, pani, po, pod, podczas, pomimo, ponad, ponieważ, powinien, powinna, powinni, powinno, poza, prawie, przecież, przed, przede, przedtem, przez, przy, roku, również, sama, są, się, skąd, sobie, sobą, sposób, swoje, ta, tak, taka, taki, takie, także, tam, te, tego, tej, temu, ten, teraz, też, to, tobą, tobie, toteż, trzeba, tu, tutaj, twoi, twoim, twoja, twoje, twym, twój, ty, tych, tylko, tym, u, w, wam, wami, was, wasz, wasza, wasze, we, według, wiele, wielu, więc, więcej, wszyscy, wszystkich, wszystkie, wszystkim, wszystko, wtedy, wy, właśnie, z, za, zapewne, zawsze, ze, zł, znowu, znów, został, żaden, żadna, żadne, żadnych, że, żeby, you, so, in, lt, me, xx, xd, is, gt, of, and, this, ll, the, x, d, ok, czym, km, are, tt, на, tą, oo,as, a, la, es, there, oh, ik, h, deze, dm, cc, xddd, xxx, xdd, o.o, an, amp, aaa, xdd, y, i'm, i'll, oj, etc, haha, rt, r, raz, ej, oo, go, lol, de, cet, takich, een, it, ew, hahaha, oki, but, with, sa, it's, at, en, c, 1k, for, that, about, że1, juz, aw, b, mamy, what, sie, eh, tez, bez, cie, will, hi, jesteś, same, da, razu, very, be, can't, from, będę, mogę, chcę, trochę, chciałabym, inaczej, robi, lepiej, chce, chyba, możesz, much, muszę, dobra, dobre, musze, your, nadal, cos,jakis, moją, bym, se, znow, byłam, bylo, cos, mozemy, możemy, jestesmy, jesteśmy, tyle, bede, jakąś, jakas, jakąs, jakaś, mojego, et, min, as, where, jakis, follow"
```

```{r,cache=TRUE}
stopwords <- stri_split_regex(stopwords,", ")[[1]]

data <- na.omit(data[!data%in%stopwords])
head(sort(table(data), decreasing = TRUE),20)
tail(sort(table(data), decreasing = TRUE),20)
```

Usuniemy również liczby, jak i z rzadka występujące słowa ~~nasmarowane~~ pisane orientalnymi alfabetami.

```{r,cache=TRUE}
data <- data[stri_detect_regex(data,"[a-ż]")]
slowa <- sort(table(data), decreasing = FALSE)
```

Zadość estetyce uczyni pakiet *wordcount*.

```{r}
wordcloud(names(slowa),slowa,scale=c(4,.2), colors="blue")
```

Skąd użytkownicy Twittera pisali najczęściej? (I skąd ~~u licha~~ pochodzą te orientalne zdania?)

```{r}
library(ggplot2)
library(grid)
map.data <- map_data("world")
lon <- na.omit(as.numeric(parsedTwees$lon))
lat <-  na.omit(as.numeric(parsedTwees$lat))
points <- data.frame(x = lon, y =lat)
points <- points[points$y > 25, ]
ggplot(map.data) + geom_map(aes(map_id = region), map = map.data, fill = "white", 
                            color = "grey20", size = 0.25) + expand_limits(x = range(lon), y = range(lat)) + 
      theme(axis.line = element_blank(), axis.text = element_blank(), axis.ticks = element_blank(), 
            axis.title = element_blank(), panel.background = element_blank(), panel.border = element_blank(), 
            panel.grid.major = element_blank(), plot.background = element_blank(), 
            plot.margin = unit(0 * c(-1.5, -1.5, -1.5, -1.5), "lines")) + geom_point(data = points, 
                                                                                     aes(x = x, y = y), size = 1, alpha = 1/5, color = "darkblue")

```

Widzimy, że najchętniej twitują ludzie z okolic stolic.
Które polskie województw twitują najczęściej?
(Opracowano na podstawie twitów z większego z plików.)

```{r,echo=FALSE,cache=TRUE}

miasta <- parsedTwees[parsedTwees$country_code=="PL",]$full_name
wojewodztwa <- sort(table(na.omit(unlist(stri_extract_last_regex(miasta, "(?<=, ).+")))))
wojewodztwa <- wojewodztwa[-c(1:9,11,12,16,20)]  # usuwam wojewodztwa w stylu: "Polen","Polska"
par(mar = c(10,3,3,3))
barplot(wojewodztwa,las=2)

```